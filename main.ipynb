{
 "metadata": {
  "name": "",
  "signature": "sha256:0d85d6c39e800a044054e0dbd182260a72cda19b38f4c8751cf6429c92e80c6d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import argparse\n",
      "import cPickle\n",
      "import lasagne\n",
      "import numpy as np\n",
      "import os\n",
      "import pyprind\n",
      "import re\n",
      "import sys\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import time\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "from sklearn.cross_validation import *\n",
      "from sklearn.decomposition import *\n",
      "from sklearn.metrics import *\n",
      "from sklearn.preprocessing import *\n",
      "from theano.ifelse import ifelse\n",
      "from theano.printing import Print as pp\n",
      "from calculate_wups import dirac_measure, fuzzy_set_membership_measure, items2list, score_it, wup_measure\n",
      "from utils import *\n",
      "\n",
      "USE_SINGLE_ANSWER = True\n",
      "BATCH_SIZE = 512\n",
      "FEATURES_FILE = 'vgg/features.pkl'\n",
      "STOPWORDS = stopwords.words('english')\n",
      "\n",
      "TRAIN_FILE = 'data/daquar37/qa.37.raw.train.txt'\n",
      "TEST_FILE = 'data/daquar37/qa.37.raw.test.txt'\n",
      "\n",
      "W2V_FILE = 'embeddings/word2vec/GoogleNews-vectors-negative300.bin'\n",
      "GLOVE_FILE = 'embeddings/glove/glove.840B.300d.txt'\n",
      "\n",
      "TYPOS = {\n",
      "    'clockes': 'clocks',\n",
      "    'toyhouse': 'toy house',\n",
      "    'benhind': 'behind',\n",
      "    'squer': 'square',\n",
      "    'liquod': 'liquid',\n",
      "    'tshirtsg': 'tshirts',\n",
      "    'firepalce': 'fireplace',\n",
      "    'corck': 'cork',\n",
      "    'viisble': 'visible',\n",
      "    'cupbaord': 'cupboard',\n",
      "    'eimage999': 'image999',\n",
      "    'beneatht': 'beneath',\n",
      "    'inbetweeng': 'in between',\n",
      "    'airconditionerg': 'air conditioner',\n",
      "    'objests': 'objects',\n",
      "    'objest': 'object'\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 745\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MAX_LEN = 0\n",
      "\n",
      "def clean(s):\n",
      "    if re.match('image[0-9]+', s):\n",
      "        return 'image'\n",
      "    if s in TYPOS:\n",
      "        return TYPOS[s]\n",
      "    return s\n",
      "\n",
      "def escapeNumber(line):\n",
      "    line = re.sub('^21$', 'twenty_one', line)\n",
      "    line = re.sub('^22$', 'twenty_two', line)\n",
      "    line = re.sub('^23$', 'twenty_three', line)\n",
      "    line = re.sub('^24$', 'twenty_four', line)\n",
      "    line = re.sub('^25$', 'twenty_five', line)\n",
      "    line = re.sub('^26$', 'twenty_six', line)\n",
      "    line = re.sub('^27$', 'twenty_seven', line)\n",
      "    line = re.sub('^28$', 'twenty_eight', line)\n",
      "    line = re.sub('^29$', 'twenty_nine', line)\n",
      "    line = re.sub('^30$', 'thirty', line)\n",
      "    line = re.sub('^11$', 'eleven', line)\n",
      "    line = re.sub('^12$', 'twelve', line)\n",
      "    line = re.sub('^13$', 'thirteen', line)\n",
      "    line = re.sub('^14$', 'fourteen', line)\n",
      "    line = re.sub('^15$', 'fifteen', line)\n",
      "    line = re.sub('^16$', 'sixteen', line)\n",
      "    line = re.sub('^17$', 'seventeen', line)\n",
      "    line = re.sub('^18$', 'eighteen', line)\n",
      "    line = re.sub('^19$', 'nineteen', line)\n",
      "    line = re.sub('^20$', 'twenty', line)\n",
      "    line = re.sub('^10$', 'ten', line)\n",
      "    line = re.sub('^0$', 'zero', line)\n",
      "    line = re.sub('^1$', 'one', line)\n",
      "    line = re.sub('^2$', 'two', line)\n",
      "    line = re.sub('^3$', 'three', line)\n",
      "    line = re.sub('^4$', 'four', line)\n",
      "    line = re.sub('^5$', 'five', line)\n",
      "    line = re.sub('^6$', 'six', line)\n",
      "    line = re.sub('^7$', 'seven', line)\n",
      "    line = re.sub('^8$', 'eight', line)\n",
      "    line = re.sub('^9$', 'nine', line)\n",
      "    return line\n",
      "\n",
      "def extract_qa(lines):\n",
      "    global MAX_LEN\n",
      "    questions = []\n",
      "    answers = []\n",
      "    imgIds = []\n",
      "    lineMax = 0\n",
      "    for i in range(0, len(lines) // 2):\n",
      "        n = i * 2\n",
      "        if ',' in lines[n + 1]:\n",
      "            # No multiple words answer for now.\n",
      "            continue\n",
      "        match = re.search('image(\\d+)', lines[n])\n",
      "        number = int((re.search('\\d+', match.group())).group())\n",
      "        line = lines[n]\n",
      "        line = re.sub(' in the image(\\d+)( \\?\\s)?', '' , line)\n",
      "        line = re.sub(' in this image(\\d+)( \\?\\s)?', '' , line)\n",
      "        line = re.sub(' on the image(\\d+)( \\?\\s)?', '' , line)\n",
      "        line = re.sub(' of the image(\\d+)( \\?\\s)?', '' , line)\n",
      "        line = re.sub(' in image(\\d+)( \\?\\s)?', '' , line)\n",
      "        line = re.sub(' image(\\d+)( \\?\\s)?', '' , line)\n",
      "        words = [clean(w) for w in line.split()]\n",
      "        line = ' '.join(words)\n",
      "        MAX_LEN = max(MAX_LEN, len(words))\n",
      "        questions.append(line)\n",
      "        answer = escapeNumber(re.sub('\\s$', '', lines[n + 1]))\n",
      "        answers.append(answer)\n",
      "        # Important! Here image id is 0-based.\n",
      "        imgIds.append(number - 1)\n",
      "    return (questions, answers, imgIds)\n",
      "\n",
      "def data_split(data, imgids, split):\n",
      "    td = []\n",
      "    vd = []\n",
      "    for (d, i) in zip(data, imgids):\n",
      "        if split[i] == 0:\n",
      "            vd.append(d)\n",
      "        else:\n",
      "            td.append(d)\n",
      "    return (td, vd)\n",
      "\n",
      "def train_valid_split(imgids):\n",
      "    split = {}\n",
      "    for i in imgids:\n",
      "        split[i] = 1\n",
      "    count = 0\n",
      "    for i in split.keys():\n",
      "        if count < len(split) / 10:\n",
      "            split[i] = 0\n",
      "        else:\n",
      "            break\n",
      "        count += 1\n",
      "    return split\n",
      "\n",
      "def load_file(fname):\n",
      "    global MAX_LEN\n",
      "    L = [line.strip() for line in open(fname)]\n",
      "    return extract_qa(L)\n",
      "\n",
      "    Q = L[0::2]\n",
      "    A = [y.split(', ') for y in L[1::2]]\n",
      "    I = []\n",
      "    for i,x in enumerate(Q):\n",
      "        words = x.split()        \n",
      "        img_id = int(re.sub('[^0-9]', '', words[-2]))-1        \n",
      "        I.append(img_id)\n",
      "        words = words[:-4]\n",
      "        words = [clean(w) for w in words]\n",
      "        Q[i] = ' '.join(words)\n",
      "        MAX_LEN = max(len(Q[i].split()), MAX_LEN)\n",
      "    if USE_SINGLE_ANSWER:\n",
      "        Q, A, I = [list(t) for t in zip(*filter(lambda z: len(z[1]) == 1, zip(Q, A, I)))]\n",
      "        A = [z[0] for z in A]\n",
      "    return Q, A, I\n",
      "\n",
      "X_train_val, Y_train_val_raw, I_train_val = load_file(TRAIN_FILE)\n",
      "X_test, Y_test_raw, I_test = load_file(TEST_FILE)\n",
      "\n",
      "split = train_valid_split(I_train_val)\n",
      "X_train, X_val = data_split(X_train_val, I_train_val, split)\n",
      "Y_train_raw, Y_val_raw = data_split(Y_train_val_raw, I_train_val, split)\n",
      "I_train, I_val = data_split(I_train_val, I_train_val, split)\n",
      "\n",
      "\"\"\"\n",
      "X_train, X_val, Y_train_raw, Y_val_raw, I_train, I_val = train_test_split(X_train_val,\n",
      "                                                                          Y_train_val_raw,\n",
      "                                                                          I_train_val,\n",
      "                                                                          test_size=512,\n",
      "                                                                          random_state=42)\n",
      "\"\"\"                                                                          \n",
      "\n",
      "dataset = { 'train': { 'q': X_train, 'y': Y_train_raw, 'img_idxs': I_train },\n",
      "            'val': { 'q': X_val, 'y': Y_val_raw, 'img_idxs': I_val },\n",
      "            'test': { 'q': X_test, 'y': Y_test_raw, 'img_idxs': I_test } }\n",
      "\n",
      "for data in dataset.keys():\n",
      "    for key in dataset[data].keys():\n",
      "        print data, key, len(dataset[data][key])\n",
      "        dataset[data][key] = pad_to_batch_size(dataset[data][key], BATCH_SIZE)\n",
      "        print data, key, len(dataset[data][key])\n",
      "\n",
      "if USE_SINGLE_ANSWER:\n",
      "    mlb = LabelBinarizer()\n",
      "else:\n",
      "    mlb = MultiLabelBinarizer()\n",
      "mlb.fit(Y_train_val_raw + Y_test_raw)\n",
      "\n",
      "for data in dataset.keys():\n",
      "    dataset[data]['y'] = mlb.transform(dataset[data]['y'])\n",
      "    \n",
      "print \"MAX_LEN: \", MAX_LEN\n",
      "print \"ANSWERS: \", len(mlb.classes_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test q 3284\n",
        "test q 3584\n",
        "test y 3284\n",
        "test y 3584\n",
        "test img_idxs 3284\n",
        "test img_idxs 3584\n",
        "train q 3336\n",
        "train q 3584\n",
        "train y 3336\n",
        "train y 3584\n",
        "train img_idxs 3336\n",
        "train img_idxs 3584\n",
        "val q 489\n",
        "val q 512\n",
        "val y 489\n",
        "val y 512\n",
        "val img_idxs 489\n",
        "val img_idxs 512\n",
        "MAX_LEN:  28\n",
        "ANSWERS:  68\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GradClip(theano.compile.ViewOp):\n",
      "\n",
      "    def __init__(self, clip_lower_bound, clip_upper_bound):\n",
      "        self.clip_lower_bound = clip_lower_bound\n",
      "        self.clip_upper_bound = clip_upper_bound\n",
      "        assert(self.clip_upper_bound >= self.clip_lower_bound)\n",
      "\n",
      "    def grad(self, args, g_outs):\n",
      "        def pgrad(g_out):\n",
      "            g_out = T.clip(g_out, self.clip_lower_bound, self.clip_upper_bound)\n",
      "            g_out = ifelse(T.any(T.isnan(g_out)), T.ones_like(g_out)*0.00001, g_out)\n",
      "            return g_out\n",
      "        return [pgrad(g_out) for g_out in g_outs]\n",
      "\n",
      "gradient_clipper = GradClip(-10.0, 10.0)\n",
      "#T.opt.register_canonicalize(theano.gof.OpRemove(gradient_clipper), name='gradient_clipper')\n",
      "\n",
      "def adam(loss, all_params, learning_rate=0.001, b1=0.9, b2=0.999, e=1e-8,\n",
      "         gamma=1-1e-8):\n",
      "    \"\"\"\n",
      "    ADAM update rules\n",
      "    Default values are taken from [Kingma2014]\n",
      "\n",
      "    References:\n",
      "    [Kingma2014] Kingma, Diederik, and Jimmy Ba.\n",
      "    \"Adam: A Method for Stochastic Optimization.\"\n",
      "    arXiv preprint arXiv:1412.6980 (2014).\n",
      "    http://arxiv.org/pdf/1412.6980v4.pdf\n",
      "\n",
      "    \"\"\"\n",
      "    updates = []\n",
      "    all_grads = theano.grad(gradient_clipper(loss), all_params)\n",
      "    alpha = learning_rate\n",
      "    t = theano.shared(np.float32(1))\n",
      "    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n",
      " \n",
      "    for theta_previous, g in zip(all_params, all_grads):\n",
      "        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
      "                                            dtype=theano.config.floatX))\n",
      "        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
      "                                            dtype=theano.config.floatX))\n",
      " \n",
      "        m = b1_t*m_previous + (1 - b1_t)*g                             # (Update biased first moment estimate)\n",
      "        v = b2*v_previous + (1 - b2)*g**2                              # (Update biased second raw moment estimate)\n",
      "        m_hat = m / (1-b1**t)                                          # (Compute bias-corrected first moment estimate)\n",
      "        v_hat = v / (1-b2**t)                                          # (Compute bias-corrected second raw moment estimate)\n",
      "        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n",
      " \n",
      "        updates.append((m_previous, m))\n",
      "        updates.append((v_previous, v))\n",
      "        updates.append((theta_previous, theta) )\n",
      "    updates.append((t, t + 1.))\n",
      "    return updates\n",
      "\n",
      "class Model:\n",
      "    def __init__(self,\n",
      "                 data,\n",
      "                 U,\n",
      "                 V,\n",
      "                 num_output,\n",
      "                 img_w=300,\n",
      "                 hidden_size=100,\n",
      "                 batch_size=32,\n",
      "                 lr=0.001,\n",
      "                 fine_tune_W=True,\n",
      "                 optimizer='adam',\n",
      "                 use_lstm=True,\n",
      "                 is_bidirectional=False):\n",
      "        self.data = data\n",
      "        self.batch_size = batch_size\n",
      "        img_h = MAX_LEN\n",
      "        \n",
      "        USE_IMG_FEATS = True\n",
      "        offset = 1 if USE_IMG_FEATS else 0\n",
      "        \n",
      "        index = T.iscalar() \n",
      "        q = T.imatrix('q')\n",
      "        y = T.imatrix('y')        \n",
      "        img_idxs = T.ivector('img_idxs')\n",
      "        q_seqlen = T.ivector('q_seqlen')\n",
      "        img_features = theano.shared(V, name='img_features', borrow=True)\n",
      "        embeddings = theano.shared(U, name='embeddings', borrow=True)\n",
      "        zero_vec_tensor = T.fvector()\n",
      "        self.zero_vec = np.zeros(img_w, dtype=theano.config.floatX)\n",
      "        self.set_zero = theano.function([zero_vec_tensor], updates=[(embeddings, T.set_subtensor(embeddings[0,:], zero_vec_tensor))])\n",
      "        \n",
      "        q_input = embeddings[q.flatten()].reshape((q.shape[0], q.shape[1], embeddings.shape[1]))\n",
      "        \n",
      "        l_in = lasagne.layers.InputLayer(shape=(batch_size, img_h+offset, img_w))        \n",
      "        if is_bidirectional:\n",
      "            raise 'Bidirectional unsupported'\n",
      "        else:\n",
      "            if use_lstm:\n",
      "                l_dropout = lasagne.layers.DropoutLayer(l_in, p=0.5)\n",
      "                l_recurrent = lasagne.layers.LSTMLayer(l_dropout,\n",
      "                                                       hidden_size,\n",
      "                                                       W_in_to_ingate=lasagne.init.Normal(0.01),\n",
      "                                                       W_hid_to_ingate=lasagne.init.Normal(0.01),\n",
      "                                                       W_cell_to_ingate=lasagne.init.Normal(0.01),\n",
      "                                                       W_in_to_forgetgate=lasagne.init.Normal(0.01),\n",
      "                                                       W_hid_to_forgetgate=lasagne.init.Normal(0.01),\n",
      "                                                       W_cell_to_forgetgate=lasagne.init.Normal(0.01),\n",
      "                                                       W_in_to_cell=lasagne.init.Normal(0.01),\n",
      "                                                       W_hid_to_cell=lasagne.init.Normal(0.01),\n",
      "                                                       W_in_to_outgate=lasagne.init.Normal(0.01),\n",
      "                                                       W_hid_to_outgate=lasagne.init.Normal(0.01),\n",
      "                                                       W_cell_to_outgate=lasagne.init.Normal(0.01),\n",
      "                                                       backwards=False,\n",
      "                                                       learn_init=False,\n",
      "                                                       peepholes=False)\n",
      "            else:\n",
      "                raise 'RNN Unsupported'\n",
      "        \n",
      "        l_combined_in = lasagne.layers.InputLayer(shape=(batch_size, hidden_size))\n",
      "        if USE_SINGLE_ANSWER:\n",
      "            l_out = lasagne.layers.DenseLayer(l_combined_in,\n",
      "                                              num_units=num_output,\n",
      "                                              W=lasagne.init.Uniform(0.025),\n",
      "                                              nonlinearity=lasagne.nonlinearities.softmax)\n",
      "        else:\n",
      "            l_out = lasagne.layers.DenseLayer(l_combined_in,\n",
      "                                              num_units=num_output,\n",
      "                                              W=lasagne.init.Uniform(0.025),\n",
      "                                              nonlinearity=lasagne.nonlinearities.sigmoid)\n",
      "        if USE_IMG_FEATS:            \n",
      "            l_imgfeats_in = lasagne.layers.InputLayer(shape=(batch_size, img_features.get_value().shape[1]))\n",
      "            l_transform = lasagne.layers.DenseLayer(l_imgfeats_in,\n",
      "                                                    num_units=embeddings.get_value().shape[1],\n",
      "                                                    W=lasagne.init.Uniform(0.025),\n",
      "                                                    nonlinearity=None)\n",
      "        \n",
      "            e_imgfeats = l_transform.get_output(img_features[img_idxs]).reshape((batch_size, 1, embeddings.get_value().shape[1]))        \n",
      "            q_input = T.concatenate([q_input, e_imgfeats], axis=1)\n",
      "        e_question = l_recurrent.get_output(q_input, deterministic=False)[T.arange(batch_size), q_seqlen+offset].reshape((q.shape[0], hidden_size))        \n",
      "        e_question_det = l_recurrent.get_output(q_input, deterministic=True)[T.arange(batch_size), q_seqlen+offset].reshape((q.shape[0], hidden_size))        \n",
      "        probas = l_out.get_output(e_question, determinstic=False)\n",
      "        probas = T.clip(probas, 1e-7, 1.0-1e-7)\n",
      "        probas_det = l_out.get_output(e_question_det, determinstic=True)\n",
      "        probas_det = T.clip(probas_det, 1e-7, 1.0-1e-7)\n",
      "                \n",
      "        cost = T.nnet.binary_crossentropy(probas, y).sum(axis=1).mean()\n",
      "       \n",
      "        params = lasagne.layers.get_all_params(l_out) + lasagne.layers.get_all_params(l_recurrent)\n",
      "        if USE_IMG_FEATS:\n",
      "            params += lasagne.layers.get_all_params(l_transform)\n",
      "        if fine_tune_W:\n",
      "            params += [embeddings]\n",
      "            \n",
      "        if 'adam' == optimizer:\n",
      "            updates = adam(cost, params, learning_rate=lr)\n",
      "        else:\n",
      "            raise 'Unsupported optimizer'\n",
      "            \n",
      "        self.shared_data = {}        \n",
      "        self.shared_data['q'] = theano.shared(np.zeros((batch_size, MAX_LEN), dtype=np.int32))\n",
      "        self.shared_data['y'] = theano.shared(np.zeros((batch_size, num_output), dtype=np.int32))\n",
      "        for key in ['q_seqlen', 'img_idxs']:\n",
      "            self.shared_data[key] = theano.shared(np.zeros((batch_size,), dtype=np.int32))            \n",
      "\n",
      "        givens = {\n",
      "            q: self.shared_data['q'],\n",
      "            y: self.shared_data['y'],\n",
      "            q_seqlen: self.shared_data['q_seqlen'],\n",
      "            img_idxs: self.shared_data['img_idxs']\n",
      "        }\n",
      "        self.train_model = theano.function([], cost, updates=updates, givens=givens, on_unused_input='warn')\n",
      "        self.get_probas = theano.function([], probas_det, givens=givens, on_unused_input='warn')\n",
      "        \n",
      "    def get_batch(self, dataset, index, max_l=MAX_LEN):\n",
      "        seqlen = np.zeros((self.batch_size,), dtype=np.int32)\n",
      "        batch = np.zeros((self.batch_size, max_l), dtype=np.int32)\n",
      "        data = dataset[index*self.batch_size:(index+1)*self.batch_size]\n",
      "        for i,row in enumerate(data):\n",
      "            row = row[:max_l]\n",
      "            batch[i,0:len(row)] = row\n",
      "            seqlen[i] = len(row)-1\n",
      "        return batch, seqlen\n",
      "    \n",
      "    def set_shared_variables(self, dataset, index):\n",
      "        q, q_seqlen = self.get_batch(dataset['q'], index)        \n",
      "        num_rows = len(dataset['img_idxs'][index*self.batch_size:(index+1)*self.batch_size])\n",
      "        \n",
      "        img_idxs = np.zeros((self.batch_size,), dtype=np.int32)\n",
      "        y = np.zeros((self.batch_size, dataset['y'].shape[1]), dtype=np.int32)\n",
      "        img_idxs[:num_rows] = dataset['img_idxs'][index*self.batch_size:(index+1)*self.batch_size]\n",
      "        y[:num_rows] = dataset['y'][index*self.batch_size:(index+1)*self.batch_size]\n",
      "        \n",
      "        self.shared_data['q'].set_value(q)\n",
      "        self.shared_data['y'].set_value(y)\n",
      "        self.shared_data['q_seqlen'].set_value(q_seqlen)\n",
      "        self.shared_data['img_idxs'].set_value(img_idxs)\n",
      "\n",
      "    def compute_probas(self, dataset, index):\n",
      "        self.set_shared_variables(dataset, index)\n",
      "        return self.get_probas()\n",
      "    \n",
      "    def compute_wups_score(self, input_gt, input_pred, thresh):\n",
      "        if thresh == -1:\n",
      "            our_element_membership = dirac_measure\n",
      "        else:\n",
      "            our_element_membership = lambda x, y: wup_measure(x, y, thresh)\n",
      "        our_set_membership = lambda x, A: fuzzy_set_membership_measure(x, A, our_element_membership)\n",
      "        score_list = [score_it(items2list(ta), items2list(pa), our_set_membership) for (ta, pa) in zip(input_gt, input_pred)]\n",
      "        return float(sum(score_list)) / float(len(score_list))\n",
      "    \n",
      "    def compute_wups_scores(self, probas, y, thresholds=[0.9, 0.0]):\n",
      "        y_pred = np.argmax(probas, axis=1)\n",
      "        y_true = np.argmax(y, axis=1)\n",
      "        y_pred_labels = [mlb.classes_[k] for k in y_pred]\n",
      "        y_true_labels = [mlb.classes_[k] for k in y_true]\n",
      "        \n",
      "        return [self.compute_wups_score(y_true_labels, y_pred_labels, thresh) for thresh in thresholds]\n",
      "    \n",
      "    def compute_accuracy_score(self, probas, y):\n",
      "        y_pred = np.argmax(probas, axis=1)\n",
      "        y_true = np.argmax(y, axis=1)\n",
      "        return np.sum(y_pred == y_true) / len(y_pred)\n",
      "        \n",
      "    def train(self, n_epochs=100, shuffle_batch=False):\n",
      "        epoch = 0\n",
      "        best_val_acc = 0\n",
      "        \n",
      "        n_train_batches = len(self.data['train']['y']) // self.batch_size\n",
      "        n_val_batches = len(self.data['val']['y']) // self.batch_size\n",
      "        n_test_batches = int(np.ceil(len(self.data['test']['y']) // self.batch_size))\n",
      "\n",
      "        while (epoch < n_epochs):\n",
      "            epoch += 1\n",
      "            indices = range(n_train_batches)\n",
      "            if shuffle_batch:\n",
      "                indices = np.random.permutation(indices)\n",
      "            #bar = pyprind.ProgBar(len(indices), monitor=True)\n",
      "            total_cost = 0\n",
      "            start_time = time.time()\n",
      "            for minibatch_index in indices:\n",
      "                self.set_shared_variables(self.data['train'], minibatch_index)\n",
      "                cost_epoch = self.train_model()\n",
      "                total_cost += cost_epoch\n",
      "                self.set_zero(self.zero_vec)\n",
      "                #bar.update()\n",
      "            end_time = time.time()\n",
      "            print \"cost: \", (total_cost / len(indices)), \" took: %d(s)\" % (end_time - start_time)\n",
      "            train_probas = np.concatenate([self.compute_probas(self.data['train'], i) for i in xrange(n_train_batches)])\n",
      "            train_acc = self.compute_accuracy_score(train_probas, self.data['train']['y'])\n",
      "\n",
      "            val_probas = np.concatenate([self.compute_probas(self.data['val'], i) for i in xrange(n_val_batches)])\n",
      "            val_acc = self.compute_accuracy_score(val_probas, self.data['val']['y'])\n",
      "            print 'epoch: %i' % epoch\n",
      "            print 'train_acc: %f' % (train_acc)\n",
      "            print 'val_acc: %f' % (val_acc)\n",
      "            if val_acc >= best_val_acc:\n",
      "                best_val_acc = val_acc\n",
      "                test_probas = np.concatenate([self.compute_probas(self.data['test'], i) for i in xrange(n_test_batches)])\n",
      "                test_acc = self.compute_accuracy_score(test_probas, self.data['test']['y'])\n",
      "                print '************************* test_acc: %f' % (test_acc)\n",
      "            else:\n",
      "                pass\n",
      "        #test_wups09, test_wups00 = self.compute_wups_scores(test_probas, self.data['test']['y'])\n",
      "        #print 'test_wups0.9: %f, test_wups0.0: %f' % (test_wups09, test_wups00)\n",
      "        \n",
      "        return test_acc, test_wups09, test_wups00\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = Counter()\n",
      "for line in X_train_val + X_test:\n",
      "    words = line.split()\n",
      "    for word in words:\n",
      "        vocab[word] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "USE_GLOVE = True\n",
      "if USE_GLOVE:\n",
      "    if os.path.isfile('glove_embeddings.pkl'):\n",
      "        glove_embeddings = cPickle.load(open('glove_embeddings.pkl'))\n",
      "    else:\n",
      "        glove_embeddings = load_glove_vec(GLOVE_FILE, vocab)\n",
      "        cPickle.dump(glove_embeddings, open('glove_embeddings.pkl', 'wb'))\n",
      "    W, word_idx_map = get_W(glove_embeddings, k=300)\n",
      "else:\n",
      "    rand_vecs = {}\n",
      "    add_unknown_words(rand_vecs, vocab, min_df=1)\n",
      "    W, word_idx_map = get_W(rand_vecs, k=300)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key in dataset.keys():\n",
      "    data = dataset[key]['q']\n",
      "    for i in xrange(len(data)):\n",
      "        if type(data[i]) == list:\n",
      "            continue\n",
      "        data[i] = get_idx_from_sent(data[i], word_idx_map, k=300)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = cPickle.load(open(FEATURES_FILE))\n",
      "print features.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1449, 4096)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Model(dataset,\n",
      "              W.astype(theano.config.floatX),\n",
      "              features.astype(theano.config.floatX),\n",
      "              num_output=len(mlb.classes_),\n",
      "              img_w=300,\n",
      "              hidden_size=150,\n",
      "              batch_size=BATCH_SIZE,\n",
      "              lr=0.001,\n",
      "              fine_tune_W=True,\n",
      "              optimizer='adam',\n",
      "              use_lstm=True,\n",
      "              is_bidirectional=False\n",
      "              )\n",
      "model.train(10000, shuffle_batch=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "QQ,AA,II=load_file(TRAIN_FILE)\n",
      "binarizer_train = LabelBinarizer()\n",
      "binarizer_train.fit(AA)\n",
      "print len(binarizer_train.classes_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "QQ_test,AA_test,II_test=load_file(TEST_FILE)\n",
      "binarizer_test = LabelBinarizer()\n",
      "binarizer_test.fit(AA_test)\n",
      "print len(binarizer_test.classes_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"train answers: \", len(set(binarizer_train.classes_))\n",
      "print \"test answers: \", len(set(binarizer_test.classes_))\n",
      "print \"total answers: \", len(set(binarizer_train.classes_) | set(binarizer_test.classes_))\n",
      "print \"only in train: \", set(binarizer_train.classes_).difference(set(binarizer_test.classes_))\n",
      "print \"only in test:  \", set(binarizer_test.classes_).difference(set(binarizer_train.classes_))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
